{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from modules.UNet import *\n",
    "from modules.Discriminator import *\n",
    "from modules.DataSet import *\n",
    "from modules.Losses import *\n",
    "\n",
    "from flownet2 import models\n",
    "\n",
    "import os,sys\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from PIL import Image,ImageEnhance\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_mask(label,flow):\n",
    "    #print(flow.shape)\n",
    "    #print(label.shape)\n",
    "    label = np.array(label)\n",
    "    edge_points = []\n",
    "    result = np.zeros_like(label)\n",
    "    for i in range(label.shape[0]):\n",
    "        curr_row = label[i,:]\n",
    "        if any(curr_row>0):\n",
    "            lnon_zero = np.where(curr_row>0)[0]\n",
    "            edge_points.append([i,lnon_zero[0]])\n",
    "            edge_points.append([i,lnon_zero[-1]])\n",
    "\n",
    "    for j in range(label.shape[1]):\n",
    "        curr_col = label[:,j]\n",
    "        if any(curr_col>0):\n",
    "            lnon_zero = np.where(curr_col>0)[0]\n",
    "            edge_points.append([lnon_zero[0],j])\n",
    "            edge_points.append([lnon_zero[-1],j])\n",
    "\n",
    "    for coord in edge_points:\n",
    "        [dx,dy] = flow[:,coord[0],coord[1]]\n",
    "        coord = (coord+np.array([dy,dx])).astype(np.long)\n",
    "        \n",
    "        result[coord[0],coord[1]] = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_OF(net,flownet,device,val_per=0.1,epochs=10,batch_size=10,resize_to=None):\n",
    "\n",
    "    transform_image = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5,0.5)\n",
    "    ])\n",
    "\n",
    "        \n",
    "    dataSet = UltraSoundDataSet_OF(root_dir,transform_image,resize_to)\n",
    "    nTrain = int(len(dataSet)*(1-val_per))\n",
    "    nValid = int(len(dataSet)-nTrain)\n",
    "    \n",
    "    trainSet,validSet = random_split(dataSet,[nTrain,nValid])\n",
    "    \n",
    "    train_loader = DataLoader(trainSet,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    valid_loader = DataLoader(validSet,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=10)\n",
    "    \n",
    "    running_loss_seg = 0\n",
    "    \n",
    "    step = 0\n",
    "    np.set_printoptions(precision=2)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            im, images_curr, labels_prev, labels_curr = batch\n",
    "                \n",
    "            #calculate optical flow\n",
    "            im = im.to(device)\n",
    "            with torch.no_grad():\n",
    "                flow = flownet(im)\n",
    "\n",
    "            flow = np.array(flow.cpu())\n",
    "            \n",
    "            labels_curr_of = np.zeros_like(labels_curr)\n",
    "            for n in range(im.shape[0]):\n",
    "                #estimate vessel position by using the flow\n",
    "                labels_curr_of[n,...] = get_next_mask(labels_prev[n,0,...],flow[n,...])\n",
    "                \n",
    "            labels_curr_of = torch.Tensor(labels_curr_of).to(device,dtype=torch.float32)\n",
    "            images_curr = images_curr.to(device)\n",
    "            labels_curr = labels_curr.to(device)\n",
    "#             print(images_curr.shape)\n",
    "#             print(labels_curr_of.shape)\n",
    "            if np.random.rand(1)>0.5:\n",
    "                pred = net(images_curr,labels_curr_of, mode=1)\n",
    "                print(\"+\",end='')\n",
    "            else:\n",
    "                pred = net(images_curr,labels_curr_of, mode=0)\n",
    "                print(\"-\",end='')\n",
    "                \n",
    "            seg_loss = DiceLoss(pred,labels_curr)\n",
    "            #print(seg_loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            seg_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_seg += seg_loss.item()\n",
    "            \n",
    "            step += 1    \n",
    "            if step % 10 == 9:    # print every 10 mini-batches\n",
    "                print()\n",
    "                print('[%d, %5d] loss: %.3f' %(epoch + 1, step + 1, running_loss_seg / 10))\n",
    "                running_loss_seg = 0.0\n",
    "                \n",
    "            if step%50 == 49:\n",
    "                val_loss = 0\n",
    "                for batch in valid_loader:\n",
    "                    im, images_curr, labels_prev, labels_curr = batch\n",
    "                    \n",
    "                    #calculate optical flow\n",
    "                    im = im.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        #print(im.shape)\n",
    "                        flow = flownet(im)\n",
    "                        #print(flow.shape)\n",
    "\n",
    "                    flow = np.array(flow.cpu())\n",
    "                        \n",
    "                    labels_curr_of = np.zeros_like(labels_curr)\n",
    "                    for n in range(im.shape[0]):\n",
    "                        #estimate vessel position by using the flow   \n",
    "                        labels_curr_of[n,...] = get_next_mask(labels_prev[n,0,...],flow[n,...])\n",
    "                    \n",
    "                    labels_curr_of = torch.Tensor(labels_curr_of).to(device,dtype=torch.float32)\n",
    "                    labels_curr = labels_curr.to(device)\n",
    "                    images_curr = torch.Tensor(images_curr).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        pred = net(images_curr,labels_curr_of,mode=0)\n",
    "\n",
    "                    val_loss += DiceLoss(pred,labels_curr)\n",
    "                print('[%d, %5d] validation loss: %.3f' %(epoch + 1, step + 1, val_loss / len(valid_loader)))\n",
    "                scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#print(\"use \",device)\n",
    "unet = UNet_OF(init_features=64).to(device)\n",
    "\n",
    "class FlowNet2Args():\n",
    "    fp16 = False\n",
    "    rgb_max = 255\n",
    "\n",
    "args = FlowNet2Args()\n",
    "flownet = models.FlowNet2(args).cuda()\n",
    "flownet.load_state_dict(torch.load('/home/zhenyuli/Downloads/FlowNet2_checkpoint.pth.tar')[\"state_dict\"])\n",
    "flownet = flownet.eval()\n",
    "\n",
    "#IMG_SIZE = [256,256]\n",
    "\n",
    "root_dir = os.path.expanduser(\"~/workspace/us_robot/DataSet/realDataSet/linear/vessel_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenyuli/.local/lib/python3.6/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--++\n",
      "[1,    10] loss: 0.872\n",
      "++--++---+\n",
      "[1,    20] loss: 0.954\n",
      "+-+-+-+++-\n",
      "[1,    30] loss: 0.948\n",
      "++-+-++++-\n",
      "[1,    40] loss: 0.932\n",
      "++-++--++-\n",
      "[1,    50] loss: 0.902\n",
      "[1,    50] validation loss: 0.916\n",
      "----+-+--+\n",
      "[1,    60] loss: 0.889\n",
      "+++--+-++-\n",
      "[1,    70] loss: 0.860\n",
      "+--++---+-\n",
      "[1,    80] loss: 0.824\n",
      "-++-+-+---\n",
      "[1,    90] loss: 0.768\n",
      "++++-+++-+\n",
      "[1,   100] loss: 0.616\n",
      "[1,   100] validation loss: 0.666\n",
      "----+-+-+-\n",
      "[1,   110] loss: 0.543\n",
      "--+--+++--\n",
      "[2,   120] loss: 0.398\n",
      "--+++--++-\n",
      "[2,   130] loss: 0.299\n",
      "++--++---+\n",
      "[2,   140] loss: 0.227\n",
      "--++------\n",
      "[2,   150] loss: 0.186\n",
      "[2,   150] validation loss: 0.205\n",
      "-+-+-+--++\n",
      "[2,   160] loss: 0.148\n",
      "-+-++-----\n",
      "[2,   170] loss: 0.128\n",
      "++++----++\n",
      "[2,   180] loss: 0.157\n",
      "-+-+++-+++\n",
      "[2,   190] loss: 0.103\n",
      "-++++++++-\n",
      "[2,   200] loss: 0.101\n",
      "[2,   200] validation loss: 0.129\n",
      "-++-+--++-\n",
      "[2,   210] loss: 0.104\n",
      "+++----++-\n",
      "[2,   220] loss: 0.095\n",
      "---++"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_OF(unet,flownet,device,val_per=0.1,epochs=2,batch_size=10,resize_to=None)\n",
    "except KeyboardInterrupt:\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for python\n",
    "torch.save(unet.state_dict(), './unet_of_usseg.pth')\n",
    "# #for c++\n",
    "# traced_script_module = torch.jit.trace(unet, img)\n",
    "# traced_script_module.save(\"./unet_of_usseg_traced.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.expanduser(\"~/workspace/us_robot/DataSet/realDataSet/linear/vessel_test2\")\n",
    "pred_dir = os.path.expanduser(\"~/workspace/us_robot/DataSet/realDataSet/linear/vessel_pred\")\n",
    "\n",
    "testset_list = os.listdir(test_dir)\n",
    "testset_list = sorted(list(filter(lambda x: x.endswith('png'), testset_list)))\n",
    "resize_to=[256,256]\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize(resize_to),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5,0.5) #Division by 255 is done, when the transformation assumes an image.\n",
    "    ])\n",
    "transform_label = transforms.Compose([\n",
    "    transforms.Resize(resize_to),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "invtransform_label = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize([512,512])\n",
    "    ])\n",
    "\n",
    "for sample in testset_list:\n",
    "    image_path = os.path.join(test_dir,sample)\n",
    "    label_path = os.path.join(pred_dir,sample)\n",
    "    \n",
    "    img = Image.open(image_path).convert(\"L\")\n",
    "    img = ImageEnhance.Contrast(img).enhance(1.5)\n",
    "    img = transform_image(img).unsqueeze(0)\n",
    "    labels_curr_of = torch.Tensor(np.zeros_like(img))\n",
    "    img = img.to(device)\n",
    "    labels_curr_of = labels_curr_of.to(device)\n",
    "    #label = Image.open(label_path)\n",
    "    #label = transform_label(label).to(device)\n",
    "    #label = label.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = unet(img,labels_curr_of, mode=0)\n",
    "    \n",
    "    #DiceIndex = (1 - DiceLoss(pred,label)).cpu().item()\n",
    "\n",
    "    pred = invtransform_label(pred.cpu().squeeze(0))\n",
    "    #fname = \"pred%.2f.png\"%DiceIndex\n",
    "    fname = sample\n",
    "    sav_path = os.path.join(pred_dir,fname)\n",
    "    pred.save(sav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
