{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from modules.UNet import *\n",
    "from modules.Discriminator import *\n",
    "from modules.DataSet import *\n",
    "from modules.Losses import *\n",
    "\n",
    "from flownet2 import models\n",
    "\n",
    "import os,sys\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from PIL import Image,ImageEnhance\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_mask_edge(label,flow):\n",
    "    #print(flow.shape)\n",
    "    #print(label.shape)\n",
    "    label = np.array(label)\n",
    "    edge_points = []\n",
    "    result = np.zeros_like(label)\n",
    "    for i in range(label.shape[0]):\n",
    "        curr_row = label[i,:]\n",
    "        if any(curr_row>0):\n",
    "            lnon_zero = np.where(curr_row>0)[0]\n",
    "            edge_points.append([i,lnon_zero[0]])\n",
    "            edge_points.append([i,lnon_zero[-1]])\n",
    "\n",
    "    for j in range(label.shape[1]):\n",
    "        curr_col = label[:,j]\n",
    "        if any(curr_col>0):\n",
    "            lnon_zero = np.where(curr_col>0)[0]\n",
    "            edge_points.append([lnon_zero[0],j])\n",
    "            edge_points.append([lnon_zero[-1],j])\n",
    "\n",
    "    for coord in edge_points:\n",
    "        [dx,dy] = flow[:,coord[0],coord[1]]\n",
    "        coord = (coord+np.array([dy,dx])).astype(np.long)\n",
    "        coord[0] = np.clip(coord[0],0,label.shape[0]-1)\n",
    "        coord[1] = np.clip(coord[1],0,label.shape[1]-1)\n",
    "        \n",
    "        result[coord[0],coord[1]] = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_mask(label,flow):\n",
    "    #print(flow.shape)\n",
    "    #print(label.shape)\n",
    "    label = np.array(label)\n",
    "    result = np.zeros_like(label)\n",
    "    \n",
    "    rows,cols = np.where(label>0.5)\n",
    "\n",
    "    for i in range(len(cols)):\n",
    "        \n",
    "        [dcol,drow] = flow[:,rows[i],cols[i]]\n",
    "        \n",
    "        tmp_row = np.clip(int(rows[i]+drow),0,label.shape[0]-1)\n",
    "        tmp_col = np.clip(int(cols[i]+dcol),0,label.shape[1]-1)\n",
    "        \n",
    "        result[tmp_row,tmp_col] = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_OF(net,flownet,device,val_per=0.1,epochs=10,batch_size=10,resize_to=None):\n",
    "\n",
    "    transform_image = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5,0.5)\n",
    "    ])\n",
    "\n",
    "        \n",
    "    dataSet = UltraSoundDataSet_OF(root_dir,transform_image,resize_to)\n",
    "    nTrain = int(len(dataSet)*(1-val_per))\n",
    "    nValid = int(len(dataSet)-nTrain)\n",
    "    \n",
    "    trainSet,validSet = random_split(dataSet,[nTrain,nValid])\n",
    "    \n",
    "    train_loader = DataLoader(trainSet,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    valid_loader = DataLoader(validSet,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=10)\n",
    "    \n",
    "    running_loss_seg = 0\n",
    "    \n",
    "    step = 0\n",
    "    np.set_printoptions(precision=2)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            im, images_curr, labels_prev, labels_curr = batch\n",
    "                \n",
    "            #calculate optical flow\n",
    "            im = im.to(device)\n",
    "            with torch.no_grad():\n",
    "                flow = flownet(im)\n",
    "\n",
    "            flow = np.array(flow.cpu())\n",
    "            \n",
    "            labels_curr_of = np.zeros_like(labels_curr)\n",
    "            for n in range(im.shape[0]):\n",
    "                #estimate vessel position by using the flow\n",
    "                labels_curr_of[n,...] = get_next_mask(labels_prev[n,0,...],flow[n,...])\n",
    "                \n",
    "            labels_curr_of = torch.Tensor(labels_curr_of).to(device,dtype=torch.float32)\n",
    "            images_curr = images_curr.to(device)\n",
    "            labels_curr = labels_curr.to(device)\n",
    "#             print(images_curr.shape)\n",
    "#             print(labels_curr_of.shape)\n",
    "            if np.random.rand(1)>0.5:\n",
    "                pred = net(images_curr,labels_curr_of, mode=1)\n",
    "                print(\"+\",end='')\n",
    "            else:\n",
    "                pred = net(images_curr,labels_curr_of, mode=0)\n",
    "                print(\"-\",end='')\n",
    "                \n",
    "            seg_loss = DiceLoss(pred,labels_curr)\n",
    "            #print(seg_loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            seg_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_seg += seg_loss.item()\n",
    "            \n",
    "            step += 1    \n",
    "            if step % 10 == 9:    # print every 10 mini-batches\n",
    "                print()\n",
    "                print('[%d, %5d] loss: %.3f' %(epoch + 1, step + 1, running_loss_seg / 10))\n",
    "                running_loss_seg = 0.0\n",
    "                \n",
    "            if step%50 == 49:\n",
    "                net.eval()\n",
    "                val_loss = 0\n",
    "                for batch in valid_loader:\n",
    "                    im, images_curr, labels_prev, labels_curr = batch\n",
    "                    \n",
    "                    #calculate optical flow\n",
    "                    im = im.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        #print(im.shape)\n",
    "                        flow = flownet(im)\n",
    "                        #print(flow.shape)\n",
    "\n",
    "                    flow = np.array(flow.cpu())\n",
    "                        \n",
    "                    labels_curr_of = np.zeros_like(labels_curr)\n",
    "                    for n in range(im.shape[0]):\n",
    "                        #estimate vessel position by using the flow   \n",
    "                        labels_curr_of[n,...] = get_next_mask(labels_prev[n,0,...],flow[n,...])\n",
    "                    \n",
    "                    labels_curr_of = torch.Tensor(labels_curr_of).to(device,dtype=torch.float32)\n",
    "                    labels_curr = labels_curr.to(device)\n",
    "                    images_curr = torch.Tensor(images_curr).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        pred = net(images_curr,labels_curr_of,mode=0)\n",
    "\n",
    "                    val_loss += DiceLoss(pred,labels_curr)\n",
    "                print('[%d, %5d] validation loss: %.3f' %(epoch + 1, step + 1, val_loss / len(valid_loader)))\n",
    "                scheduler.step(val_loss)\n",
    "                net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#print(\"use \",device)\n",
    "unet = UNet_OF2(init_features=64).to(device)\n",
    "\n",
    "class FlowNet2Args():\n",
    "    fp16 = False\n",
    "    rgb_max = 255\n",
    "\n",
    "args = FlowNet2Args()\n",
    "flownet = models.FlowNet2(args).cuda()\n",
    "flownet.load_state_dict(torch.load(os.path.expanduser(\"~/Downloads/FlowNet2_checkpoint.pth.tar\"))[\"state_dict\"])\n",
    "flownet = flownet.eval()\n",
    "\n",
    "#IMG_SIZE = [256,256]\n",
    "\n",
    "# root_dir = os.path.expanduser(\"~/workspace/us_robot/DataSet/realDataSet/linear/vessel_dataset\")\n",
    "root_dir = os.path.expanduser(\"~/workspace/us_robot/DataSet/phantomDataset/vessel_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eadu/workspace/us_robot/network/modules/DataSet.py:67: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(open(root_dir+'/config.yaml'))\n",
      "/home/eadu/.local/lib/python3.6/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---+++-++\n",
      "[1,    10] loss: 0.745\n",
      "-+++++--++\n",
      "[1,    20] loss: 0.593\n",
      "+++-+++-+-\n",
      "[1,    30] loss: 0.238\n",
      "---+++----\n",
      "[1,    40] loss: 0.275\n",
      "-++++++++-\n",
      "[1,    50] loss: 0.099\n",
      "[1,    50] validation loss: 0.255\n",
      "+++-++--++\n",
      "[1,    60] loss: 0.132\n",
      "+--+----++\n",
      "[1,    70] loss: 0.149\n",
      "+--++---+-\n",
      "[1,    80] loss: 0.165\n",
      "++-+++--++\n",
      "[1,    90] loss: 0.076\n",
      "--+++++-+-\n",
      "[1,   100] loss: 0.145\n",
      "[1,   100] validation loss: 0.479\n",
      "-++-+-++-+\n",
      "[2,   110] loss: 0.211\n",
      "----++--+-\n",
      "[2,   120] loss: 0.185\n",
      "--+++++---\n",
      "[2,   130] loss: 0.141\n",
      "++----+-++\n",
      "[2,   140] loss: 0.089\n",
      "-+--++----\n",
      "[2,   150] loss: 0.112\n",
      "[2,   150] validation loss: 0.104\n",
      "--+-+--+-+\n",
      "[2,   160] loss: 0.082\n",
      "--+--+-++-\n",
      "[2,   170] loss: 0.061\n",
      "-+---+-+++\n",
      "[2,   180] loss: 0.053\n",
      "+---+-+++-\n",
      "[2,   190] loss: 0.069\n",
      "------+---\n",
      "[2,   200] loss: 0.077\n",
      "[2,   200] validation loss: 0.057\n",
      "-++--+++--\n",
      "[3,   210] loss: 0.039\n",
      "+--+-+-+++\n",
      "[3,   220] loss: 0.057\n",
      "+------+++\n",
      "[3,   230] loss: 0.062\n",
      "+-------+-\n",
      "[3,   240] loss: 0.053\n",
      "----++++-+\n",
      "[3,   250] loss: 0.042\n",
      "[3,   250] validation loss: 0.042\n",
      "+-++-+++-+\n",
      "[3,   260] loss: 0.045\n",
      "-+-----++-\n",
      "[3,   270] loss: 0.046\n",
      "-+-+---+-+\n",
      "[3,   280] loss: 0.038\n",
      "-+++++++-+\n",
      "[3,   290] loss: 0.028\n",
      "+-++--++++\n",
      "[3,   300] loss: 0.024\n",
      "[3,   300] validation loss: 0.039\n",
      "-++-------\n",
      "[4,   310] loss: 0.033\n",
      "----+-+-+-\n",
      "[4,   320] loss: 0.030\n",
      "++-++-+-++\n",
      "[4,   330] loss: 0.027\n",
      "---+--++-+\n",
      "[4,   340] loss: 0.026\n",
      "+-++-+-+-+\n",
      "[4,   350] loss: 0.036\n",
      "[4,   350] validation loss: 0.034\n",
      "+++-+-+---\n",
      "[4,   360] loss: 0.029\n",
      "+++--+++-+\n",
      "[4,   370] loss: 0.025\n",
      "++++--++++\n",
      "[4,   380] loss: 0.023\n",
      "+--+-+++++\n",
      "[4,   390] loss: 0.025\n",
      "-++-+---+-\n",
      "[4,   400] loss: 0.026\n",
      "[4,   400] validation loss: 0.033\n",
      "+----++---\n",
      "[5,   410] loss: 0.025\n",
      "-++--++-+-\n",
      "[5,   420] loss: 0.032\n",
      "+---++-++-\n",
      "[5,   430] loss: 0.031\n",
      "-+-+++---+\n",
      "[5,   440] loss: 0.023\n",
      "++--+-+++-\n",
      "[5,   450] loss: 0.023\n",
      "[5,   450] validation loss: 0.038\n",
      "+-+---++-+\n",
      "[5,   460] loss: 0.023\n",
      "-+++-+----\n",
      "[5,   470] loss: 0.024\n",
      "-+++-++--+\n",
      "[5,   480] loss: 0.031\n",
      "++-+++----\n",
      "[5,   490] loss: 0.023\n",
      "--++--+--+\n",
      "[5,   500] loss: 0.029\n",
      "[5,   500] validation loss: 0.029\n",
      "----+--+-+\n",
      "[5,   510] loss: 0.025\n",
      "-+++------\n",
      "[6,   520] loss: 0.029\n",
      "+-+++++-++\n",
      "[6,   530] loss: 0.018\n",
      "+----+----\n",
      "[6,   540] loss: 0.028\n",
      "-++---+-++\n",
      "[6,   550] loss: 0.022\n",
      "[6,   550] validation loss: 0.026\n",
      "--++---+--\n",
      "[6,   560] loss: 0.022\n",
      "-++-+++-+-\n",
      "[6,   570] loss: 0.023\n",
      "+--+-+----\n",
      "[6,   580] loss: 0.024\n",
      "-+-++++--+\n",
      "[6,   590] loss: 0.025\n",
      "---+++++++\n",
      "[6,   600] loss: 0.093\n",
      "[6,   600] validation loss: 0.041\n",
      "-+-+-+----\n",
      "[6,   610] loss: 0.111\n",
      "++++---+-+\n",
      "[7,   620] loss: 0.052\n",
      "+++-++----\n",
      "[7,   630] loss: 0.031\n",
      "+++-++---+\n",
      "[7,   640] loss: 0.027\n",
      "+----++++-\n",
      "[7,   650] loss: 0.030\n",
      "[7,   650] validation loss: 0.031\n",
      "++++-+-+-+\n",
      "[7,   660] loss: 0.023\n",
      "----+---++\n",
      "[7,   670] loss: 0.035\n",
      "+-+--+++-+\n",
      "[7,   680] loss: 0.026\n",
      "----++++-+\n",
      "[7,   690] loss: 0.028\n",
      "++--++++++\n",
      "[7,   700] loss: 0.020\n",
      "[7,   700] validation loss: 0.029\n",
      "+++-+-+++-\n",
      "[7,   710] loss: 0.026\n",
      "-++--+++-+\n",
      "[8,   720] loss: 0.026\n",
      "++-++-++++\n",
      "[8,   730] loss: 0.021\n",
      "++--+----+\n",
      "[8,   740] loss: 0.026\n",
      "+-++--+--+\n",
      "[8,   750] loss: 0.026\n",
      "[8,   750] validation loss: 0.026\n",
      "---+--+-++\n",
      "[8,   760] loss: 0.022\n",
      "-+-+--+--+\n",
      "[8,   770] loss: 0.022\n",
      "-+++-++-+-\n",
      "[8,   780] loss: 0.019\n",
      "++++++-+--\n",
      "[8,   790] loss: 0.024\n",
      "+++-+--++-\n",
      "[8,   800] loss: 0.020\n",
      "[8,   800] validation loss: 0.024\n",
      "++---++++-\n",
      "[8,   810] loss: 0.032\n",
      "-+--+-++-+\n",
      "[9,   820] loss: 0.020\n",
      "----++++++\n",
      "[9,   830] loss: 0.020\n",
      "+-+----+--\n",
      "[9,   840] loss: 0.032\n",
      "---++-+---\n",
      "[9,   850] loss: 0.026\n",
      "[9,   850] validation loss: 0.023\n",
      "---+++---+\n",
      "[9,   860] loss: 0.021\n",
      "--++--+++-\n",
      "[9,   870] loss: 0.029\n",
      "--+++--+++\n",
      "[9,   880] loss: 0.020\n",
      "+-++--+-++\n",
      "[9,   890] loss: 0.019\n",
      "+-+-+---++\n",
      "[9,   900] loss: 0.023\n",
      "[9,   900] validation loss: 0.022\n",
      "+--+++-+-+\n",
      "[9,   910] loss: 0.024\n",
      "----++-+--\n",
      "[10,   920] loss: 0.021\n",
      "+--+--+++-\n",
      "[10,   930] loss: 0.028\n",
      "----+++-++\n",
      "[10,   940] loss: 0.024\n",
      "----------\n",
      "[10,   950] loss: 0.021\n",
      "[10,   950] validation loss: 0.022\n",
      "+++++-++--\n",
      "[10,   960] loss: 0.020\n",
      "-+-+--++++\n",
      "[10,   970] loss: 0.024\n",
      "+--+--++--\n",
      "[10,   980] loss: 0.031\n",
      "+-----+-+-\n",
      "[10,   990] loss: 0.028\n",
      "-++----+--\n",
      "[10,  1000] loss: 0.021\n",
      "[10,  1000] validation loss: 0.023\n",
      "+-+-+-++--\n",
      "[10,  1010] loss: 0.021\n",
      "+---++-+-+\n",
      "[10,  1020] loss: 0.025\n",
      "+"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_OF(unet,flownet,device,val_per=0.1,epochs=10,batch_size=10,resize_to=None)\n",
    "except KeyboardInterrupt:\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for python\n",
    "torch.save(unet.state_dict(), './unet_of_usseg_phantom.pth')\n",
    "# #for c++\n",
    "# traced_script_module = torch.jit.trace(unet, img)\n",
    "# traced_script_module.save(\"./unet_of_usseg_traced.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "PATH = os.path.expanduser(\"~/workspace/us_robot/network/unet_of_usseg.pth\")\n",
    "unet = UNet_OF(init_features=64).to(device)\n",
    "unet.load_state_dict(torch.load(PATH))\n",
    "unet = unet.eval()\n",
    "\n",
    "class FlowNet2Args():\n",
    "    fp16 = False\n",
    "    rgb_max = 255\n",
    "\n",
    "args = FlowNet2Args()\n",
    "flownet = models.FlowNet2(args).cuda()\n",
    "flownet.load_state_dict(torch.load('/home/zhenyuli/Downloads/FlowNet2_checkpoint.pth.tar')[\"state_dict\"])\n",
    "flownet = flownet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = unet.eval()\n",
    "flownet = flownet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.expanduser(\"~/workspace/us_robot/DataSet/realDataSet/linear/vessel_test2/img\")\n",
    "pred_dir = os.path.expanduser(\"~/workspace/us_robot/DataSet/realDataSet/linear/vessel_pred\")\n",
    "\n",
    "testset_list = os.listdir(test_dir)\n",
    "testset_list = sorted(list(filter(lambda x: x.endswith('png'), testset_list)))\n",
    "resize_to=[256,256]\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize(resize_to),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5,0.5) #Division by 255 is done, when the transformation assumes an image.\n",
    "    ])\n",
    "\n",
    "invtransform_label = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    #transforms.Resize([512,512])\n",
    "    ])\n",
    "\n",
    "for sample in testset_list:\n",
    "    image_path = os.path.join(test_dir,sample)\n",
    "    label_path = os.path.join(pred_dir,sample)\n",
    "    \n",
    "    img = Image.open(image_path).convert(\"L\")\n",
    "    #img = ImageEnhance.Contrast(img).enhance(1.5)\n",
    "    img = transform_image(img).unsqueeze(0)\n",
    "    labels_curr_of = torch.Tensor(np.zeros_like(img))\n",
    "    img = img.to(device)\n",
    "    labels_curr_of = labels_curr_of.to(device)\n",
    "    #label = Image.open(label_path)\n",
    "    #label = transform_label(label).to(device)\n",
    "    #label = label.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = unet(img,labels_curr_of, mode=0)\n",
    "    \n",
    "    #DiceIndex = (1 - DiceLoss(pred,label)).cpu().item()\n",
    "\n",
    "    pred = invtransform_label(pred.cpu().squeeze(0))\n",
    "    #fname = \"pred%.2f.png\"%DiceIndex\n",
    "    fname = sample\n",
    "    sav_path = os.path.join(pred_dir,fname)\n",
    "    pred.save(sav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenyuli/.local/lib/python3.6/site-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation loss: 0.145\n"
     ]
    }
   ],
   "source": [
    "test_dir = os.path.expanduser(\"~/workspace/us_robot/DataSet/realDataSet/linear/vessel_test2\")\n",
    "pred_dir = os.path.expanduser(\"~/workspace/us_robot/DataSet/realDataSet/linear/vessel_pred2\")\n",
    "\n",
    "transform_image = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5,0.5)\n",
    "    ])\n",
    "invtransform_label = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    #transforms.Resize([512,512])\n",
    "    ])\n",
    "\n",
    "#eval_dataSet = UltraSoundDataSet_OF(test_dir,transform_image,resize_to=[256,256])\n",
    "eval_dataSet = UltraSoundDataSet_OF(test_dir,transform_image)\n",
    "eval_loader = DataLoader(eval_dataSet,batch_size=1,shuffle=False,num_workers=4)\n",
    "\n",
    "eval_loss = 0\n",
    "for idx,batch in enumerate(eval_loader):\n",
    "    im, images_curr, labels_prev_init, labels_curr = batch\n",
    "    \n",
    "    #calculate optical flow\n",
    "    im = im.to(device)\n",
    "    with torch.no_grad():\n",
    "        flow = flownet(im)\n",
    "\n",
    "    flow = np.array(flow.cpu())\n",
    "\n",
    "    labels_curr_of = np.zeros_like(labels_curr)\n",
    "\n",
    "    #estimate vessel position by using the flow\n",
    "    if idx == 0:\n",
    "        labels_curr_of[0,...] = get_next_mask(labels_prev_init[0,0,...],flow[0,...])\n",
    "    else:        \n",
    "        labels_curr_of[0,...] = get_next_mask(labels_prev[0,0,...],flow[0,...])\n",
    "\n",
    "    labels_curr_of = torch.Tensor(labels_curr_of).to(device,dtype=torch.float32)\n",
    "    labels_curr = labels_curr.type(torch.float32).to(device)\n",
    "    images_curr = images_curr.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = unet(images_curr,labels_curr_of,mode=int(idx>0))\n",
    "        #pred = unet(images_curr,labels_curr_of,mode=0)\n",
    "    \n",
    "    eval_loss += DiceLoss(pred,labels_curr)\n",
    "    \n",
    "    labels_prev = pred.cpu()\n",
    "    \n",
    "    pred = invtransform_label(pred.cpu().squeeze(0))\n",
    "    #fname = \"pred%.2f.png\"%DiceIndex\n",
    "    fname = \"pred%03d.png\" %idx\n",
    "    sav_path = os.path.join(pred_dir,fname)\n",
    "    pred.save(sav_path)\n",
    "    \n",
    "print('evaluation loss: %.3f' %(eval_loss / len(eval_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.expanduser(\"~/workspace/us_robot/DataSet/realDataSet/linear/vessel_dataset\")\n",
    "transform_image = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5,0.5)\n",
    "    ])\n",
    "\n",
    "eval_dataSet = UltraSoundDataSet_OF(test_dir,transform_image)\n",
    "eval_loader = DataLoader(eval_dataSet,batch_size=1,shuffle=False,num_workers=4)\n",
    "batch = next(iter(eval_loader))\n",
    "im, images_curr, labels_prev_init, labels_curr = batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
