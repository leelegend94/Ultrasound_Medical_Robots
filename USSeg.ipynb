{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os,sys\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self,in_channels=1,out_channels=1,init_features=64):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet.DoubleConv2d(in_channels,features)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.encoder2 = UNet.DoubleConv2d(features,2*features)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder3 = UNet.DoubleConv2d(2*features,4*features)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.encoder4 = UNet.DoubleConv2d(4*features,8*features)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.bottleneck = UNet.DoubleConv2d(8*features,16*features)\n",
    "        \n",
    "        self.upconv4 = nn.ConvTranspose2d(16*features,8*features,kernel_size=2,stride=2)        \n",
    "        self.decoder4 = UNet.DoubleConv2d(16*features,8*features) #concate, 2*8=16\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(8*features,4*features,kernel_size=2,stride=2)\n",
    "        self.decoder3 = UNet.DoubleConv2d(8*features,4*features) #concate, 2*4=8\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(4*features,2*features,kernel_size=2,stride=2)\n",
    "        self.decoder2 = UNet.DoubleConv2d(4*features,2*features) #concate, 2*2=4\n",
    "        \n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(2*features,features,kernel_size=2,stride=2)\n",
    "        self.decoder1 = UNet.DoubleConv2d(2*features,features) #concate, 2*1=2\n",
    "        \n",
    "        self.conv_out = nn.Conv2d(features, 1, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        enc1 = self.encoder1(input)\n",
    "        \n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        \n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        \n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "        \n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = torch.cat([enc4,self.upconv4(bottleneck)],dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        \n",
    "        dec3 = torch.cat([enc3,self.upconv3(dec4)],dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        \n",
    "        dec2 = torch.cat([enc2,self.upconv2(dec3)],dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        \n",
    "        dec1 = torch.cat([enc1,self.upconv1(dec2)],dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        \n",
    "        output = torch.sigmoid(self.conv_out(dec1))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def DoubleConv2d(in_channels,features):\n",
    "        return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, features, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(features),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(features),\n",
    "                nn.ReLU(inplace=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.img_conv1 = nn.Conv2d(1, 16, kernel_size = 3, padding=1)\n",
    "        self.img_conv2 = nn.Conv2d(16, 64, kernel_size = 3, padding=1)\n",
    "        self.img_pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.mask_conv = nn.Conv2d(1, 64, kernel_size = 3, padding=1)\n",
    "        self.mask_pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.dis_conv1 = nn.Conv2d(128, 256, kernel_size = 3, padding=1)\n",
    "        self.dis_pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dis_conv2 = nn.Conv2d(256, 512, kernel_size = 3, padding=1)\n",
    "        self.dis_pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.output = nn.Linear(512,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, mask):\n",
    "        x = self.img_conv1(input)\n",
    "        x = self.img_conv2(x)\n",
    "        x = self.img_pool(x)\n",
    "        \n",
    "        y = self.mask_conv(mask)\n",
    "        y = self.mask_pool(y)\n",
    "        \n",
    "        x = torch.cat([x,y],dim=1)\n",
    "        x = self.dis_conv1(x)\n",
    "        x = self.dis_pool1(x)\n",
    "        x = self.dis_conv2(x)\n",
    "        x = self.dis_pool2(x)\n",
    "        x = F.max_pool2d(x,kernel_size=x.size()[2:]).squeeze()\n",
    "\n",
    "        output = torch.sigmoid(self.output(x))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltraSoundDataSet(Dataset):\n",
    "    def __init__(self, root_dir, transforms):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_list = os.listdir(root_dir)\n",
    "\n",
    "        self.transform_image, self.transform_label = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image_path = os.path.join(self.root_dir,self.sample_list[idx],\"image.png\")\n",
    "        label_path = os.path.join(self.root_dir,self.sample_list[idx],\"label.png\")\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        label = Image.open(label_path)\n",
    "        \n",
    "        if self.transform_image is not None:\n",
    "            image = self.transform_image(image)\n",
    "            \n",
    "        if self.transform_label is not None:\n",
    "            label = self.transform_label(label)\n",
    "        \n",
    "        return image,label\n",
    "        #sample = {\"image\":image,\"label\":label}\n",
    "        \n",
    "        #return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiceLoss(pred,target,slack=10):\n",
    "    \n",
    "    index = (2*torch.sum(pred*target)+slack)/(torch.sum(pred)+torch.sum(target)+slack)\n",
    "    #if torch.sum(target).item() == 0:\n",
    "    #print(\"instersection: \",torch.sum(pred*target).item())\n",
    "    #print(\"pred: \",torch.sum(pred).item())\n",
    "    #print(\"target: \",torch.sum(target).item())\n",
    "    #print(\"Index: \", index.item())\n",
    "    return 1-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,device,val_per=0.1,epochs=10,batch_size=10,resize_to=None):\n",
    "    if resize_to is not None:\n",
    "        transform_image = transforms.Compose([\n",
    "        transforms.Resize(resize_to),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5,0.5)\n",
    "        ])\n",
    "        transform_label = transforms.Compose([\n",
    "        transforms.Resize(resize_to),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    else:\n",
    "        transform_image = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5,0.5)\n",
    "        ])\n",
    "        transform_label = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "            \n",
    "    \n",
    "    dataSet = UltraSoundDataSet(root_dir,(transform_image,transform_label))\n",
    "    nTrain = int(len(dataSet)*(1-val_per))\n",
    "    nValid = int(len(dataSet)-nTrain)\n",
    "    \n",
    "    trainSet,validSet = random_split(dataSet,[nTrain,nValid])\n",
    "    \n",
    "    train_loader = DataLoader(trainSet,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    valid_loader = DataLoader(validSet,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=10) #mae: dice-index\n",
    "    \n",
    "    running_loss = 0\n",
    "    step = 0\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        for batch in train_loader:\n",
    "            imgs,labels = batch\n",
    "            \n",
    "            imgs = imgs.to(device=device,dtype=torch.float32)\n",
    "            labels = labels.to(device=device,dtype=torch.float32)\n",
    "            \n",
    "            pred = net(imgs)\n",
    "            \n",
    "            loss = DiceLoss(pred,labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            step += 1\n",
    "            \n",
    "            if step % 10 == 9:    # print every 10 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %(epoch + 1, step + 1, running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "            if step%50 == 49:\n",
    "                val_loss = 0\n",
    "                for batch in valid_loader:\n",
    "                    imgs,labels = batch\n",
    "                    imgs = imgs.to(device=device,dtype=torch.float32)\n",
    "                    labels = labels.to(device=device,dtype=torch.float32)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        pred = net(imgs)\n",
    "                    val_loss += DiceLoss(pred,labels)\n",
    "                print('[%d, %5d] validation loss: %.3f' %(epoch + 1, step + 1, val_loss / len(valid_loader)))\n",
    "                scheduler.step(val_loss)\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial(net,discriminator,device,val_per=0.1,epochs=10,batch_size=10,resize_to=None):\n",
    "    if resize_to is not None:\n",
    "        transform_image = transforms.Compose([\n",
    "        transforms.Resize(resize_to),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5,0.5)\n",
    "        ])\n",
    "        transform_label = transforms.Compose([\n",
    "        transforms.Resize(resize_to),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    else:\n",
    "        transform_image = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5,0.5)\n",
    "        ])\n",
    "        transform_label = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "            \n",
    "    \n",
    "    dataSet = UltraSoundDataSet(root_dir,(transform_image,transform_label))\n",
    "    nTrain = int(len(dataSet)*(1-val_per))\n",
    "    nValid = int(len(dataSet)-nTrain)\n",
    "    \n",
    "    trainSet,validSet = random_split(dataSet,[nTrain,nValid])\n",
    "    \n",
    "    train_loader = DataLoader(trainSet,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    valid_loader = DataLoader(validSet,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    \n",
    "    optimizer_unet = torch.optim.Adam(net.parameters())\n",
    "    optimizer_disc = torch.optim.Adam(discriminator.parameters())\n",
    "    scheduler_unet = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_unet,mode='min',patience=10) #mae: dice-index\n",
    "    scheduler_disc = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_disc,mode='min',patience=10)\n",
    "    BCELoss = nn.BCELoss()\n",
    "    \n",
    "    running_loss_seg = 0\n",
    "    running_loss_disc = 0\n",
    "    step = 0\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            imgs,labels = batch\n",
    "            \n",
    "            imgs = imgs.to(device=device,dtype=torch.float32)\n",
    "            labels = labels.to(device=device,dtype=torch.float32)\n",
    "            \n",
    "            # segmentation network\n",
    "            pred = net(imgs)\n",
    "            \n",
    "            # discriminator network\n",
    "            pred_valid = discriminator(imgs, labels)\n",
    "            pred_fake = discriminator(imgs, pred.detach())   \n",
    "            \n",
    "            # seg loss\n",
    "            seg_loss = DiceLoss(pred,labels)\n",
    "            \n",
    "            optimizer_unet.zero_grad()\n",
    "            seg_loss.backward(retain_graph=True)\n",
    "            optimizer_unet.step()\n",
    "            \n",
    "            valid = torch.Tensor(imgs.size(0), 1).fill_(1.0).to(device=device,dtype=torch.float32)\n",
    "            fake = torch.Tensor(imgs.size(0), 1).fill_(0.0).to(device=device,dtype=torch.float32)\n",
    "            \n",
    "            # discriminator loss\n",
    "            disc_loss = BCELoss(pred_valid,valid) + BCELoss(pred_fake,fake)\n",
    "\n",
    "            optimizer_disc.zero_grad()\n",
    "            disc_loss.backward(retain_graph=False)\n",
    "            optimizer_disc.step()\n",
    "            \n",
    "            running_loss_seg += seg_loss.item()\n",
    "            running_loss_disc += disc_loss.item()\n",
    "            step += 1\n",
    "            \n",
    "            if step % 10 == 9:    # print every 10 mini-batches\n",
    "                print('[%d, %5d] segmentation loss: %.3f; discrimination loss: %.3f' %(epoch + 1, step + 1, running_loss_seg / 10, running_loss_disc / 10))\n",
    "                running_loss_seg = 0.0\n",
    "                running_loss_disc = 0.0\n",
    "                \n",
    "            if step%50 == 49:\n",
    "                val_loss_seg = 0\n",
    "                val_loss_disc = 0\n",
    "                for batch in valid_loader:\n",
    "                    imgs,labels = batch\n",
    "                    imgs = imgs.to(device=device,dtype=torch.float32)\n",
    "                    labels = labels.to(device=device,dtype=torch.float32)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        pred_seg = net(imgs)\n",
    "                        pred_valid = discriminator(imgs, labels)\n",
    "                        pred_fake = discriminator(imgs, pred_seg.detach())\n",
    "                    \n",
    "                    val_loss_seg += DiceLoss(pred_seg,labels).item()\n",
    "                    valid = torch.Tensor(imgs.size(0), 1).fill_(1.0).to(device=device,dtype=torch.float32)\n",
    "                    fake = torch.Tensor(imgs.size(0), 1).fill_(0.0).to(device=device,dtype=torch.float32)\n",
    "                    val_loss_disc += (BCELoss(pred_valid,valid) + BCELoss(pred_fake,fake)).item()\n",
    "                print('[%d, %5d] validation loss(seg): %.3f; validation loss(disc): %.3f' %(epoch + 1, step + 1, val_loss_seg / len(valid_loader), val_loss_disc / len(valid_loader)))\n",
    "                scheduler_unet.step(val_loss_seg)\n",
    "                scheduler_disc.step(val_loss_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] segmentation loss: 0.850; discrimination loss: 0.699\n",
      "[1,    20] segmentation loss: 0.869; discrimination loss: 0.057\n",
      "[1,    30] segmentation loss: 0.882; discrimination loss: 0.001\n",
      "[1,    40] segmentation loss: 0.895; discrimination loss: 0.002\n",
      "[1,    50] segmentation loss: 0.844; discrimination loss: 0.003\n",
      "[1,    50] validation loss(seg): 0.808; validation loss(disc): 0.001\n",
      "[1,    60] segmentation loss: 0.808; discrimination loss: 0.002\n",
      "[1,    70] segmentation loss: 0.801; discrimination loss: 0.344\n",
      "[1,    80] segmentation loss: 0.791; discrimination loss: 1.758\n",
      "[1,    90] segmentation loss: 0.658; discrimination loss: 1.716\n",
      "[1,   100] segmentation loss: 0.713; discrimination loss: 1.993\n",
      "[1,   100] validation loss(seg): 0.655; validation loss(disc): 0.003\n",
      "[1,   110] segmentation loss: 0.724; discrimination loss: 0.135\n",
      "[1,   120] segmentation loss: 0.567; discrimination loss: 0.013\n",
      "[1,   130] segmentation loss: 0.582; discrimination loss: 0.021\n",
      "[1,   140] segmentation loss: 0.421; discrimination loss: 0.105\n",
      "[1,   150] segmentation loss: 0.517; discrimination loss: 0.008\n",
      "[1,   150] validation loss(seg): 0.421; validation loss(disc): 0.075\n",
      "[1,   160] segmentation loss: 0.348; discrimination loss: 0.015\n",
      "[1,   170] segmentation loss: 0.315; discrimination loss: 0.038\n",
      "[1,   180] segmentation loss: 0.375; discrimination loss: 0.069\n",
      "[1,   190] segmentation loss: 0.262; discrimination loss: 0.167\n",
      "[1,   200] segmentation loss: 0.395; discrimination loss: 0.948\n",
      "[1,   200] validation loss(seg): 0.228; validation loss(disc): 0.205\n",
      "[1,   210] segmentation loss: 0.202; discrimination loss: 0.781\n",
      "[1,   220] segmentation loss: 0.248; discrimination loss: 0.855\n",
      "[1,   230] segmentation loss: 0.214; discrimination loss: 0.752\n",
      "[1,   240] segmentation loss: 0.204; discrimination loss: 0.486\n",
      "[1,   250] segmentation loss: 0.180; discrimination loss: 0.800\n",
      "[1,   250] validation loss(seg): 0.151; validation loss(disc): 0.743\n",
      "[1,   260] segmentation loss: 0.105; discrimination loss: 0.482\n",
      "[1,   270] segmentation loss: 0.207; discrimination loss: 0.588\n",
      "[1,   280] segmentation loss: 0.290; discrimination loss: 0.896\n",
      "[1,   290] segmentation loss: 0.170; discrimination loss: 0.875\n",
      "[1,   300] segmentation loss: 0.383; discrimination loss: 0.654\n",
      "[1,   300] validation loss(seg): 0.135; validation loss(disc): 1.110\n",
      "[1,   310] segmentation loss: 0.145; discrimination loss: 0.753\n",
      "[1,   320] segmentation loss: 0.090; discrimination loss: 0.655\n",
      "[1,   330] segmentation loss: 0.059; discrimination loss: 0.536\n",
      "[1,   340] segmentation loss: 0.202; discrimination loss: 0.313\n",
      "[1,   350] segmentation loss: 0.168; discrimination loss: 0.566\n",
      "[1,   350] validation loss(seg): 0.110; validation loss(disc): 0.409\n",
      "[1,   360] segmentation loss: 0.061; discrimination loss: 0.828\n",
      "[1,   370] segmentation loss: 0.049; discrimination loss: 1.047\n",
      "[1,   380] segmentation loss: 0.094; discrimination loss: 1.013\n",
      "[1,   390] segmentation loss: 0.152; discrimination loss: 0.716\n",
      "[1,   400] segmentation loss: 0.082; discrimination loss: 0.566\n",
      "[1,   400] validation loss(seg): 0.075; validation loss(disc): 0.649\n",
      "[1,   410] segmentation loss: 0.076; discrimination loss: 0.459\n",
      "[1,   420] segmentation loss: 0.051; discrimination loss: 0.352\n",
      "[1,   430] segmentation loss: 0.051; discrimination loss: 0.454\n",
      "[1,   440] segmentation loss: 0.252; discrimination loss: 0.591\n",
      "[1,   450] segmentation loss: 0.104; discrimination loss: 0.360\n",
      "[1,   450] validation loss(seg): 0.120; validation loss(disc): 0.408\n",
      "[1,   460] segmentation loss: 0.086; discrimination loss: 0.391\n",
      "[1,   470] segmentation loss: 0.382; discrimination loss: 0.478\n",
      "[1,   480] segmentation loss: 0.199; discrimination loss: 0.918\n",
      "[1,   490] segmentation loss: 0.187; discrimination loss: 0.686\n",
      "[1,   500] segmentation loss: 0.162; discrimination loss: 0.528\n",
      "[1,   500] validation loss(seg): 0.109; validation loss(disc): 0.456\n",
      "[1,   510] segmentation loss: 0.234; discrimination loss: 0.525\n",
      "[1,   520] segmentation loss: 0.106; discrimination loss: 0.404\n",
      "[1,   530] segmentation loss: 0.062; discrimination loss: 0.313\n",
      "[1,   540] segmentation loss: 0.074; discrimination loss: 0.415\n",
      "[1,   550] segmentation loss: 0.068; discrimination loss: 0.657\n",
      "[1,   550] validation loss(seg): 0.125; validation loss(disc): 0.459\n",
      "[1,   560] segmentation loss: 0.071; discrimination loss: 0.505\n",
      "[1,   570] segmentation loss: 0.138; discrimination loss: 0.541\n",
      "[1,   580] segmentation loss: 0.124; discrimination loss: 0.540\n",
      "[1,   590] segmentation loss: 0.058; discrimination loss: 0.284\n",
      "[1,   600] segmentation loss: 0.153; discrimination loss: 0.525\n",
      "[1,   600] validation loss(seg): 0.080; validation loss(disc): 0.414\n",
      "[2,   610] segmentation loss: 0.194; discrimination loss: 0.490\n",
      "[2,   620] segmentation loss: 0.143; discrimination loss: 0.293\n",
      "[2,   630] segmentation loss: 0.236; discrimination loss: 0.393\n",
      "[2,   640] segmentation loss: 0.239; discrimination loss: 0.596\n",
      "[2,   650] segmentation loss: 0.086; discrimination loss: 0.589\n",
      "[2,   650] validation loss(seg): 0.095; validation loss(disc): 0.382\n",
      "[2,   660] segmentation loss: 0.169; discrimination loss: 0.365\n",
      "[2,   670] segmentation loss: 0.084; discrimination loss: 0.407\n",
      "[2,   680] segmentation loss: 0.054; discrimination loss: 0.514\n",
      "[2,   690] segmentation loss: 0.066; discrimination loss: 0.512\n",
      "[2,   700] segmentation loss: 0.045; discrimination loss: 0.556\n",
      "[2,   700] validation loss(seg): 0.080; validation loss(disc): 0.391\n",
      "[2,   710] segmentation loss: 0.081; discrimination loss: 0.502\n",
      "[2,   720] segmentation loss: 0.050; discrimination loss: 0.501\n",
      "[2,   730] segmentation loss: 0.051; discrimination loss: 0.331\n",
      "[2,   740] segmentation loss: 0.173; discrimination loss: 0.390\n",
      "[2,   750] segmentation loss: 0.155; discrimination loss: 0.324\n",
      "[2,   750] validation loss(seg): 0.062; validation loss(disc): 0.409\n",
      "[2,   760] segmentation loss: 0.044; discrimination loss: 0.147\n",
      "[2,   770] segmentation loss: 0.049; discrimination loss: 0.332\n",
      "[2,   780] segmentation loss: 0.041; discrimination loss: 0.413\n",
      "[2,   790] segmentation loss: 0.140; discrimination loss: 0.466\n",
      "[2,   800] segmentation loss: 0.041; discrimination loss: 0.394\n",
      "[2,   800] validation loss(seg): 0.113; validation loss(disc): 0.357\n",
      "[2,   810] segmentation loss: 0.091; discrimination loss: 0.369\n",
      "[2,   820] segmentation loss: 0.251; discrimination loss: 0.510\n",
      "[2,   830] segmentation loss: 0.056; discrimination loss: 0.456\n",
      "[2,   840] segmentation loss: 0.041; discrimination loss: 0.473\n",
      "[2,   850] segmentation loss: 0.041; discrimination loss: 0.381\n",
      "[2,   850] validation loss(seg): 0.094; validation loss(disc): 0.382\n",
      "[2,   860] segmentation loss: 0.046; discrimination loss: 0.577\n",
      "[2,   870] segmentation loss: 0.037; discrimination loss: 0.323\n",
      "[2,   880] segmentation loss: 0.048; discrimination loss: 0.663\n",
      "[2,   890] segmentation loss: 0.242; discrimination loss: 0.585\n",
      "[2,   900] segmentation loss: 0.232; discrimination loss: 0.698\n",
      "[2,   900] validation loss(seg): 0.085; validation loss(disc): 0.435\n",
      "[2,   910] segmentation loss: 0.147; discrimination loss: 0.501\n",
      "[2,   920] segmentation loss: 0.047; discrimination loss: 0.387\n",
      "[2,   930] segmentation loss: 0.145; discrimination loss: 0.516\n",
      "[2,   940] segmentation loss: 0.057; discrimination loss: 0.547\n",
      "[2,   950] segmentation loss: 0.041; discrimination loss: 0.347\n",
      "[2,   950] validation loss(seg): 0.072; validation loss(disc): 0.387\n",
      "[2,   960] segmentation loss: 0.090; discrimination loss: 0.518\n",
      "[2,   970] segmentation loss: 0.057; discrimination loss: 0.498\n",
      "[2,   980] segmentation loss: 0.136; discrimination loss: 0.402\n",
      "[2,   990] segmentation loss: 0.161; discrimination loss: 0.605\n",
      "[2,  1000] segmentation loss: 0.142; discrimination loss: 0.392\n",
      "[2,  1000] validation loss(seg): 0.081; validation loss(disc): 0.461\n",
      "[2,  1010] segmentation loss: 0.037; discrimination loss: 0.747\n",
      "[2,  1020] segmentation loss: 0.037; discrimination loss: 0.698\n",
      "[2,  1030] segmentation loss: 0.156; discrimination loss: 0.457\n",
      "[2,  1040] segmentation loss: 0.163; discrimination loss: 0.607\n",
      "[2,  1050] segmentation loss: 0.055; discrimination loss: 0.604\n",
      "[2,  1050] validation loss(seg): 0.063; validation loss(disc): 0.442\n",
      "[2,  1060] segmentation loss: 0.051; discrimination loss: 0.419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,  1070] segmentation loss: 0.060; discrimination loss: 0.364\n",
      "[2,  1080] segmentation loss: 0.047; discrimination loss: 0.476\n",
      "[2,  1090] segmentation loss: 0.054; discrimination loss: 0.401\n",
      "[2,  1100] segmentation loss: 0.048; discrimination loss: 0.369\n",
      "[2,  1100] validation loss(seg): 0.092; validation loss(disc): 0.371\n",
      "[2,  1110] segmentation loss: 0.143; discrimination loss: 0.469\n",
      "[2,  1120] segmentation loss: 0.069; discrimination loss: 0.276\n",
      "[2,  1130] segmentation loss: 0.044; discrimination loss: 0.400\n",
      "[2,  1140] segmentation loss: 0.037; discrimination loss: 0.549\n",
      "[2,  1150] segmentation loss: 0.113; discrimination loss: 0.596\n",
      "[2,  1150] validation loss(seg): 0.092; validation loss(disc): 0.417\n",
      "[2,  1160] segmentation loss: 0.045; discrimination loss: 0.358\n",
      "[2,  1170] segmentation loss: 0.048; discrimination loss: 0.422\n",
      "[2,  1180] segmentation loss: 0.142; discrimination loss: 0.366\n",
      "[2,  1190] segmentation loss: 0.230; discrimination loss: 0.600\n",
      "[2,  1200] segmentation loss: 0.327; discrimination loss: 0.966\n",
      "[2,  1200] validation loss(seg): 0.095; validation loss(disc): 0.404\n",
      "[3,  1210] segmentation loss: 0.102; discrimination loss: 0.637\n",
      "[3,  1220] segmentation loss: 0.053; discrimination loss: 0.474\n",
      "[3,  1230] segmentation loss: 0.055; discrimination loss: 0.562\n",
      "[3,  1240] segmentation loss: 0.038; discrimination loss: 0.358\n",
      "[3,  1250] segmentation loss: 0.230; discrimination loss: 0.354\n",
      "[3,  1250] validation loss(seg): 0.075; validation loss(disc): 0.394\n",
      "[3,  1260] segmentation loss: 0.036; discrimination loss: 0.470\n",
      "[3,  1270] segmentation loss: 0.046; discrimination loss: 0.455\n",
      "[3,  1280] segmentation loss: 0.062; discrimination loss: 0.618\n",
      "[3,  1290] segmentation loss: 0.207; discrimination loss: 0.482\n",
      "[3,  1300] segmentation loss: 0.143; discrimination loss: 0.418\n",
      "[3,  1300] validation loss(seg): 0.103; validation loss(disc): 0.369\n",
      "[3,  1310] segmentation loss: 0.138; discrimination loss: 0.470\n",
      "[3,  1320] segmentation loss: 0.063; discrimination loss: 0.237\n",
      "[3,  1330] segmentation loss: 0.061; discrimination loss: 0.487\n",
      "[3,  1340] segmentation loss: 0.055; discrimination loss: 0.429\n",
      "[3,  1350] segmentation loss: 0.154; discrimination loss: 0.425\n",
      "[3,  1350] validation loss(seg): 0.108; validation loss(disc): 0.388\n",
      "[3,  1360] segmentation loss: 0.159; discrimination loss: 0.685\n",
      "[3,  1370] segmentation loss: 0.043; discrimination loss: 0.421\n",
      "[3,  1380] segmentation loss: 0.153; discrimination loss: 0.597\n",
      "[3,  1390] segmentation loss: 0.036; discrimination loss: 0.208\n",
      "[3,  1400] segmentation loss: 0.147; discrimination loss: 0.520\n",
      "[3,  1400] validation loss(seg): 0.085; validation loss(disc): 0.411\n",
      "[3,  1410] segmentation loss: 0.039; discrimination loss: 0.514\n",
      "[3,  1420] segmentation loss: 0.138; discrimination loss: 0.334\n",
      "[3,  1430] segmentation loss: 0.046; discrimination loss: 0.690\n",
      "[3,  1440] segmentation loss: 0.047; discrimination loss: 0.396\n",
      "[3,  1450] segmentation loss: 0.059; discrimination loss: 0.424\n",
      "[3,  1450] validation loss(seg): 0.099; validation loss(disc): 0.392\n",
      "[3,  1460] segmentation loss: 0.144; discrimination loss: 0.526\n",
      "[3,  1470] segmentation loss: 0.050; discrimination loss: 0.361\n",
      "[3,  1480] segmentation loss: 0.130; discrimination loss: 0.523\n",
      "[3,  1490] segmentation loss: 0.053; discrimination loss: 0.561\n",
      "[3,  1500] segmentation loss: 0.037; discrimination loss: 0.453\n",
      "[3,  1500] validation loss(seg): 0.112; validation loss(disc): 0.395\n",
      "[3,  1510] segmentation loss: 0.052; discrimination loss: 0.400\n",
      "[3,  1520] segmentation loss: 0.146; discrimination loss: 0.473\n",
      "[3,  1530] segmentation loss: 0.148; discrimination loss: 0.587\n",
      "[3,  1540] segmentation loss: 0.052; discrimination loss: 0.166\n",
      "[3,  1550] segmentation loss: 0.131; discrimination loss: 0.407\n",
      "[3,  1550] validation loss(seg): 0.070; validation loss(disc): 0.398\n",
      "[3,  1560] segmentation loss: 0.129; discrimination loss: 0.405\n",
      "[3,  1570] segmentation loss: 0.039; discrimination loss: 0.389\n",
      "[3,  1580] segmentation loss: 0.045; discrimination loss: 0.342\n",
      "[3,  1590] segmentation loss: 0.049; discrimination loss: 0.586\n",
      "[3,  1600] segmentation loss: 0.040; discrimination loss: 0.356\n",
      "[3,  1600] validation loss(seg): 0.118; validation loss(disc): 0.372\n",
      "[3,  1610] segmentation loss: 0.035; discrimination loss: 0.456\n",
      "[3,  1620] segmentation loss: 0.041; discrimination loss: 0.455\n",
      "[3,  1630] segmentation loss: 0.131; discrimination loss: 0.495\n",
      "[3,  1640] segmentation loss: 0.056; discrimination loss: 0.442\n",
      "[3,  1650] segmentation loss: 0.126; discrimination loss: 0.249\n",
      "[3,  1650] validation loss(seg): 0.047; validation loss(disc): 0.397\n",
      "[3,  1660] segmentation loss: 0.048; discrimination loss: 0.427\n",
      "[3,  1670] segmentation loss: 0.044; discrimination loss: 0.656\n",
      "[3,  1680] segmentation loss: 0.050; discrimination loss: 0.414\n",
      "[3,  1690] segmentation loss: 0.146; discrimination loss: 0.523\n",
      "[3,  1700] segmentation loss: 0.122; discrimination loss: 0.264\n",
      "[3,  1700] validation loss(seg): 0.071; validation loss(disc): 0.415\n",
      "[3,  1710] segmentation loss: 0.032; discrimination loss: 0.314\n",
      "[3,  1720] segmentation loss: 0.098; discrimination loss: 0.517\n",
      "[3,  1730] segmentation loss: 0.125; discrimination loss: 0.540\n",
      "[3,  1740] segmentation loss: 0.036; discrimination loss: 0.559\n",
      "[3,  1750] segmentation loss: 0.036; discrimination loss: 0.364\n",
      "[3,  1750] validation loss(seg): 0.055; validation loss(disc): 0.401\n",
      "[3,  1760] segmentation loss: 0.042; discrimination loss: 0.644\n",
      "[3,  1770] segmentation loss: 0.039; discrimination loss: 0.473\n",
      "[3,  1780] segmentation loss: 0.316; discrimination loss: 0.626\n",
      "[3,  1790] segmentation loss: 0.048; discrimination loss: 0.408\n",
      "[3,  1800] segmentation loss: 0.043; discrimination loss: 0.570\n",
      "[3,  1800] validation loss(seg): 0.059; validation loss(disc): 0.418\n",
      "[4,  1810] segmentation loss: 0.042; discrimination loss: 0.459\n",
      "[4,  1820] segmentation loss: 0.031; discrimination loss: 0.493\n",
      "[4,  1830] segmentation loss: 0.039; discrimination loss: 0.467\n",
      "[4,  1840] segmentation loss: 0.135; discrimination loss: 0.593\n",
      "[4,  1850] segmentation loss: 0.040; discrimination loss: 0.482\n",
      "[4,  1850] validation loss(seg): 0.053; validation loss(disc): 0.428\n",
      "[4,  1860] segmentation loss: 0.037; discrimination loss: 0.481\n",
      "[4,  1870] segmentation loss: 0.031; discrimination loss: 0.443\n",
      "[4,  1880] segmentation loss: 0.034; discrimination loss: 0.464\n",
      "[4,  1890] segmentation loss: 0.026; discrimination loss: 0.533\n",
      "[4,  1900] segmentation loss: 0.034; discrimination loss: 0.339\n",
      "[4,  1900] validation loss(seg): 0.058; validation loss(disc): 0.420\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = UNet(init_features=32)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "net = net.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "root_dir = os.path.expanduser(\"~/workspace/us_robot/DataSet/SimDataset2\")\n",
    "try:\n",
    "    #train(net=net,device=device,resize_to=None,epochs=50,batch_size=5)\n",
    "    train_adversarial(net=net,discriminator=discriminator,device=device,resize_to=None,epochs=20,batch_size=3)\n",
    "except KeyboardInterrupt:\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.expanduser(\"~/workspace/us_robot/DataSet/SimDatasetTest2\")\n",
    "testset_list = os.listdir(test_dir)\n",
    "resize_to=None\n",
    "transform_image = transforms.Compose([\n",
    "    #transforms.Resize(resize_to),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5,0.5) #Division by 255 is done, when the transformation assumes an image.\n",
    "    ])\n",
    "transform_label = transforms.Compose([\n",
    "    #transforms.Resize(resize_to),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "invtransform_label = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    #transforms.Resize([1000,500])\n",
    "    ])\n",
    "for sample in testset_list:\n",
    "    image_path = os.path.join(test_dir,sample,\"image.png\")\n",
    "    label_path = os.path.join(test_dir,sample,\"label.png\")\n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    img = transform_image(img)\n",
    "    img = img.to(device)\n",
    "    img = img.unsqueeze(0)\n",
    "    label = Image.open(label_path)\n",
    "    label = transform_label(label).to(device)\n",
    "    label = label.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = net(img)\n",
    "    \n",
    "    DiceIndex = (1 - DiceLoss(pred,label)).cpu().item()\n",
    "\n",
    "    pred = invtransform_label(pred.cpu().squeeze(0))\n",
    "    fname = \"pred%.2f.png\"%DiceIndex\n",
    "    sav_path = os.path.join(test_dir,sample,fname)\n",
    "    pred.save(sav_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for python\n",
    "torch.save(net.state_dict(), './unet_usseg.pth')\n",
    "\n",
    "#for c++\n",
    "traced_script_module = torch.jit.trace(net, img)\n",
    "traced_script_module.save(\"./unet_usseg_traced.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.expanduser(\"~/workspace/us_robot/simulator/SimDatasetTest2\")\n",
    "testset_list = os.listdir(test_dir)\n",
    "resize_to=None\n",
    "transform_image = transforms.Compose([\n",
    "    #transforms.Resize(resize_to),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5,0.5) #Division by 255 is done, when the transformation assumes an image.\n",
    "    ])\n",
    "transform_label = transforms.Compose([\n",
    "    #transforms.Resize(resize_to),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "invtransform_label = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    #transforms.Resize([1000,500])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = '0002'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "image_path = os.path.join(test_dir,sample,\"image.png\")\n",
    "label_path = os.path.join(test_dir,sample,\"label.png\")\n",
    "pred_path = os.path.join(test_dir,sample,\"pred0.06.png\")\n",
    "\n",
    "img = Image.open(image_path)\n",
    "img = transform_image(img)\n",
    "img = img.to(device)\n",
    "img = img.unsqueeze(0)\n",
    "label = Image.open(label_path)\n",
    "label = transform_label(label).to(device)\n",
    "label = label.unsqueeze(0)\n",
    "pred = Image.open(pred_path)\n",
    "pred = transform_label(pred).to(device)\n",
    "pred = pred.unsqueeze(0)\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    pred = net(img)\n",
    "\n",
    "DiceIndex = (1 - DiceLoss(pred,label)).cpu().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = os.path.join(test_dir,sample,\"pred0.06.png\")\n",
    "pred = Image.open(pred_path)\n",
    "pred = transform_label(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([0.1])\n",
    "y = torch.Tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3026)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
